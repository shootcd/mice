################# Basic crawler

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import csv

def crawl(url, depth, visited=None, writer=None, parent="Root"):
    visited = visited or set()
    if depth == 0 or url in visited:
        return

    print(f"Crawling: {url} (Found on: {parent})")
    visited.add(url)
    if writer:
        writer.writerow([url, parent])

    try:
        res = requests.get(url, timeout=5)
        if res.status_code != 200:
            return

        soup = BeautifulSoup(res.text, 'html.parser')
        links = [urljoin(url, a['href']) for a in soup.find_all('a', href=True)]

        for link in links:
            if link.startswith("http"):
                crawl(link, depth-1, visited, writer, url)

    except requests.RequestException as e:
        print(f"Error: {e}")

# -------- Run --------
start_url = "https://www.starbucks.com/"

with open("crawled_links.csv", "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["URL", "Found On"])
    crawl(start_url, depth=2, writer=w)
